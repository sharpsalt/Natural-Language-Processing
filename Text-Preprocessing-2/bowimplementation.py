# -*- coding: utf-8 -*-
"""BOWImplementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jhV6EiOpAV-dSqJ6EempZK1ukv3wCES2
"""

import pandas as pd
import nltk
import numpy as np

dataset=pd.read_csv('./spam.csv', encoding='latin-1')

print(dataset)

dataset=dataset[['v1', 'v2']]

dataset.columns=['label','message']

print(dataset)

"""# Data Cleaning and Preprocessing"""

#Lowering the sentences,applying stopwords,lemmatization or stemmer etc i can use it

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

from nltk.stem import WordNetLemmatizer
wordlemmatize=WordNetLemmatizer()

nltk.download('wordnet')

nltk.download('stopwords')

import re

corpus=[]
print(len(dataset))

for i in range(len(dataset)):
  review=re.sub('[^a-zA-Z]',' ',dataset['message'][i])
  review=review.lower()
  review=review.split()
  review=[wordlemmatize.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]
  review=' '.join(review)
  corpus.append(review)

corpus

#We also know the problem that due to stemming some of the words might not come right but maximum number of words come right. if you want to make it more accurate then you can apply lemmatization
#humlog yahan pe snowball stemmer bhi laga skte hai

# Create the Bag of Words

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=100)

#max_features mtlb maximum feature rahega

X=cv.fit_transform(corpus).toarray()

print(X.shape)

#Currently this is Normal Bag of Words
#Now how to use Binary Bag of word
#simple hai dekho
#cv=CountVectorizer(max_features=100,binary=True)
#mtlb isme binary=True krdena hai basically to hojayega

"""# n-grams"""

cv.vocabulary_

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=100,binary=True,ngram_range=(1,1))
X=cv.fit_transform(corpus).toarray()

cv.vocabulary_

cv=CountVectorizer(max_features=200,binary=True,ngram_range=(1,2))
X=cv.fit_transform(corpus).toarray()

cv.vocabulary_

cv=CountVectorizer(max_features=100,binary=True,ngram_range=(2,2))
X=cv.fit_transform(corpus).toarray()

#If i want to see maximum occuring bigram of words like i am ignoring unigrams

cv.vocabulary_

#Pehle humlog (1,1) se start krte then 1,2 then 1,3 then play with other hyperparameter like max_feature phir uske baad 2,1 phir2,2 etc
#n-grams ranges plays a very important role in deciding accuracy

print(X)

corpus

"""# Create TF-IDF and N-grams"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(max_features=100)

type(corpus)

X=tfidf.fit_transform(corpus).toarray()

#once i do it we will want to see all the values clearly
np.set_printoptions(edgeitems=30,linewidth=100000,
                    formatter=dict(float=lambda x:"%.3g"%x))

X

#These are nothing but the words importance so i can simply do it using tf-idf vectors

"""# N-Grams"""

tfidf=TfidfVectorizer(max_features=100,ngram_range=(2,2))
X=tfidf.fit_transform(corpus).toarray()

X













