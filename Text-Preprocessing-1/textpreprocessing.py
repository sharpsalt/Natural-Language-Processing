# -*- coding: utf-8 -*-
"""TextPreProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Q16M-olOqfP6ulolqzg5ejvoXqszskI
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install nltk

corpus="""Hello Welcome, to Srijan Verma NLP Tutorials.
Please do watch the entire course to become expert in NLP"""

print(corpus)

##Tokenization

#Sentence->Paragraph

import nltk

nltk.download('punkt_tab')

from nltk.tokenize import sent_tokenize

documents=sent_tokenize(corpus)

print(type(documents))

#Paragraph->Words
#or
#Sentence->Words

from nltk.tokenize import word_tokenize
words=word_tokenize(corpus)
print(word_tokenize(corpus))

for sentences in documents:
  print(sentences)

from nltk.tokenize import wordpunct_tokenize

wordpunct_tokenize(corpus)

## This wordpunct_tokenize me punctuation bhi as a seperate word treat krta hai

from nltk.tokenize import TreebankWordTokenizer
tokenizer=TreebankWordTokenizer()

tokenizer.tokenize(corpus)

#There is very minute difference with the help of Tree Bank Tokenizer , full stop won't be treated as seperate word but for the last word full stop would be treated as seperate word

#Generic way or most of the time we use word_tokenizer etc





"""## Stemming"""

#Classification Problem
#Comments of Product is a positive review or negative review
#Reviews----> eating,eat,eaten , like yahan pe eat is the stem word
#having this variety of words won't impact the result


words=["eating","eats","eaten","writing","writes","programming","programs","history","finally","finalized"]

"""# PorterStemmer"""

from nltk.stem import PorterStemmer

stemming=PorterStemmer()
#now for each an devery word i will apply this stemming
for word in words:
  print(word+"------>"+stemming.stem(word))

"""# RegexpStemmer"""

from nltk.stem import RegexpStemmer

reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)

reg_stemmer.stem("eating")

"""# Snowball Stemmer"""

from nltk.stem import SnowballStemmer
snowballstemmer=SnowballStemmer('english')

for word in words:
  print(snowballstemmer.stem(word))



"""# Lemmatization

# 1. Wordnet Lemmatization
"""

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
lematizer=WordNetLemmatizer()

lematizer.lemmatize('eating',pos='v')



"""# StopWords"""

paragraph="I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.I have a THIRD VISION. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr.Vikram Sarabhai, of the Dept. of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life."

from nltk.stem import PorterStemmer

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

stopwords.words('english')

stemmer=PorterStemmer()

sentences=nltk.sent_tokenize(paragraph)

#Basically we took a paragraph and then converted it into sentence like we did tokeinzation

print(sentences)

type(sentences)

#since we have tokenized it, now understand , we will traverse through all the sentences and
#apply stopwords ,which all words are not present in stopwords , we will take that and apply stemming

# Apply Stopwords and Filter and then apply Stemming
for i in range(len(sentences)):
  words=nltk.word_tokenize(sentences[i]) #i will get list of words for each sentences
  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]
  sentences[i]=' '.join(words) #Converting all the list of words into sentences

sentences

#stopwords.words('english')



from nltk.stem import SnowballStemmer
snowballstemmer=SnowballStemmer('english')

# Apply Stopwords and Filter and then apply Stemming
for i in range(len(sentences)):
  words=nltk.word_tokenize(sentences[i]) #i will get list of words for each sentences
  words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]
  sentences[i]=' '.join(words) #Converting all the list of words into sentences

sentences

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

for i in range(len(sentences)):
  # sentences[i]=sentences[i].tolower()
  words=nltk.word_tokenize(sentences[i])
  words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]
  sentences[i]=' '.join(words)

sentences



"""# Parts of Speech"""

paragraph="I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.I have a THIRD VISION. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr.Vikram Sarabhai, of the Dept. of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life."

import nltk
sentences=nltk.sent_tokenize(paragraph)

print(len(sentences))

from nltk.corpus import stopwords

import nltk
nltk.download('averaged_perceptron_tagger_eng')

# We will find out te POS Tags
for i in range(len(sentences)):
  words=nltk.word_tokenize(sentences[i])
  words=[word for word in words if word not in set(stopwords.words('english'))]
  pos_tag=nltk.pos_tag(words)
  print(pos_tag)



"""# Named Entity Recognition"""

sentence="The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structure"

import nltk
words=nltk.word_tokenize(sentence)

print(words)

tag_element=nltk.pos_tag(words)

print(type(tag_element[0]))

nltk.download('maxent_ne_chunker_tab')

nltk.download('svgling')

nltk.download('words')

# Commented out IPython magic to ensure Python compatibility.
# %pip install svgling

from IPython.display import display, HTML

display(HTML("""
<style>
svg {
    background-color: white !important;
}
</style>
"""))

nltk.ne_chunk(tag_element)
#Use NLTK's currently recommended named entity chunker to chunk the given list of tagged tokens



















